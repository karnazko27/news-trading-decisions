{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fxvqaifJPw4J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxvqaifJPw4J",
    "outputId": "ab0852d7-3e22-453d-8d29-d26ba3e05ce0"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "UHhw-xKWP5au",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHhw-xKWP5au",
    "outputId": "1b54d071-5fd7-42ff-bd2b-ffcf9c0bc9e3"
   },
   "outputs": [],
   "source": [
    "# cd drive/MyDrive/IE7500_GroupB/Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099321db-5cfa-4a71-916d-fdf884735d53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "099321db-5cfa-4a71-916d-fdf884735d53",
    "outputId": "e8c0fc24-3711-4069-8272-d494f8d168ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ensorflow (/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f87f3e-f78a-4f25-9c1e-e81cae8ff0d4",
   "metadata": {
    "id": "87f87f3e-f78a-4f25-9c1e-e81cae8ff0d4"
   },
   "outputs": [],
   "source": [
    "# load necesary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c114d1-2f7d-44f0-a037-fd53cd17e1f8",
   "metadata": {
    "id": "45c114d1-2f7d-44f0-a037-fd53cd17e1f8"
   },
   "outputs": [],
   "source": [
    "dtypes_dict = {'headline': 'object',\n",
    "               'url': 'object',\n",
    "               'publisher': 'object',\n",
    "               'stock': 'object',\n",
    "               'tokens': 'object',\n",
    "               'normalized_tokens': 'object',\n",
    "               'filtered_tokens': 'object',\n",
    "               'lemmas': 'object',\n",
    "               'sentiment_score': 'float64',\n",
    "               'Name': 'object',\n",
    "               'Market Cap': 'float64',\n",
    "               'Country': 'object',\n",
    "               'IPO Year': 'float64',\n",
    "               'Sector': 'object',\n",
    "               'Industry': 'object',\n",
    "               'year': 'int32',\n",
    "               'month': 'int32',\n",
    "               'day_of_week': 'int32',\n",
    "               'sentiment_label': 'int64',\n",
    "               'headline_length': 'int64',\n",
    "               'word_count': 'int64',\n",
    "               'Market_Cap_Category': 'object',\n",
    "               'recommendation': 'object',\n",
    "               'cap_Large': 'bool',\n",
    "               'cap_Medium': 'bool',\n",
    "               'cap_Mega': 'bool',\n",
    "               'cap_Micro': 'bool',\n",
    "               'cap_Nano': 'bool',\n",
    "               'cap_Small': 'bool',\n",
    "               'sector_Basic Materials': 'bool',\n",
    "               'sector_Consumer Discretionary': 'bool',\n",
    "               'sector_Consumer Staples': 'bool',\n",
    "               'sector_Energy': 'bool',\n",
    "               'sector_Finance': 'bool',\n",
    "               'sector_Health Care': 'bool',\n",
    "               'sector_Industrials': 'bool',\n",
    "               'sector_Miscellaneous': 'bool',\n",
    "               'sector_Real Estate': 'bool',\n",
    "               'sector_Technology': 'bool',\n",
    "               'sector_Telecommunications': 'bool',\n",
    "               'sector_Utilities': 'bool', 'recommendation_label': 'int64',\n",
    "               'publisher_label': 'int64', 'country_label': 'int64',\n",
    "               'industry_label': 'int64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4fb8d8-cb25-47c5-b2b4-4f1d5b0f7e0a",
   "metadata": {
    "id": "ff4fb8d8-cb25-47c5-b2b4-4f1d5b0f7e0a"
   },
   "outputs": [],
   "source": [
    "# load dataframes to use\n",
    "df_main = pd.read_csv(\"saved_dfs/df_for_models.csv\", dtype=dtypes_dict,\n",
    "                      parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2c9cd5-42e4-406c-a58e-5b1931384587",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "fe2c9cd5-42e4-406c-a58e-5b1931384587",
    "outputId": "5e8877ba-0b01-4e39-9ca8-dbe5df0d440a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>tokens</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>...</th>\n",
       "      <th>sector_Industrials</th>\n",
       "      <th>sector_Miscellaneous</th>\n",
       "      <th>sector_Real Estate</th>\n",
       "      <th>sector_Technology</th>\n",
       "      <th>sector_Telecommunications</th>\n",
       "      <th>sector_Utilities</th>\n",
       "      <th>recommendation_label</th>\n",
       "      <th>publisher_label</th>\n",
       "      <th>country_label</th>\n",
       "      <th>industry_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agilent Technologies Announces Pricing of $5……...</td>\n",
       "      <td>http://www.gurufocus.com/news/1153187/agilent-...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>A</td>\n",
       "      <td>['Agilent', 'Technologies', 'Announces', 'Pric...</td>\n",
       "      <td>['agilent', 'technologies', 'announces', 'pric...</td>\n",
       "      <td>['agilent', 'technologies', 'announces', 'pric...</td>\n",
       "      <td>['agilent', 'technology', 'announces', 'pricin...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agilent (A) Gears Up for Q2 Earnings: What's i...</td>\n",
       "      <td>http://www.zacks.com/stock/news/931205/agilent...</td>\n",
       "      <td>Zacks</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>A</td>\n",
       "      <td>['Agilent', '(', 'A', ')', 'Gears', 'Up', 'for...</td>\n",
       "      <td>['agilent', 'a', 'gears', 'up', 'for', 'q2', '...</td>\n",
       "      <td>['agilent', 'gears', 'q2', 'earnings', 'cards']</td>\n",
       "      <td>['agilent', 'gear', 'q2', 'earnings', 'card']</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J.P. Morgan Asset Management Announces Liquida...</td>\n",
       "      <td>http://www.gurufocus.com/news/1138923/jp-morga...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>A</td>\n",
       "      <td>['J.P.', 'Morgan', 'Asset', 'Management', 'Ann...</td>\n",
       "      <td>['morgan', 'asset', 'management', 'announces',...</td>\n",
       "      <td>['morgan', 'asset', 'management', 'announces',...</td>\n",
       "      <td>['morgan', 'asset', 'management', 'announces',...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pershing Square Capital Management, L.P. Buys ...</td>\n",
       "      <td>http://www.gurufocus.com/news/1138704/pershing...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>A</td>\n",
       "      <td>['Pershing', 'Square', 'Capital', 'Management'...</td>\n",
       "      <td>['pershing', 'square', 'capital', 'management'...</td>\n",
       "      <td>['pershing', 'square', 'capital', 'management'...</td>\n",
       "      <td>['pershing', 'square', 'capital', 'management'...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agilent Awards Trilogy Sciences with a Golden ...</td>\n",
       "      <td>http://www.gurufocus.com/news/1134012/agilent-...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>A</td>\n",
       "      <td>['Agilent', 'Awards', 'Trilogy', 'Sciences', '...</td>\n",
       "      <td>['agilent', 'awards', 'trilogy', 'sciences', '...</td>\n",
       "      <td>['agilent', 'awards', 'trilogy', 'sciences', '...</td>\n",
       "      <td>['agilent', 'award', 'trilogy', 'science', 'go...</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Agilent Technologies Announces Pricing of $5……...   \n",
       "1  Agilent (A) Gears Up for Q2 Earnings: What's i...   \n",
       "2  J.P. Morgan Asset Management Announces Liquida...   \n",
       "3  Pershing Square Capital Management, L.P. Buys ...   \n",
       "4  Agilent Awards Trilogy Sciences with a Golden ...   \n",
       "\n",
       "                                                 url  publisher       date  \\\n",
       "0  http://www.gurufocus.com/news/1153187/agilent-...  GuruFocus 2020-06-01   \n",
       "1  http://www.zacks.com/stock/news/931205/agilent...      Zacks 2020-05-18   \n",
       "2  http://www.gurufocus.com/news/1138923/jp-morga...  GuruFocus 2020-05-15   \n",
       "3  http://www.gurufocus.com/news/1138704/pershing...  GuruFocus 2020-05-15   \n",
       "4  http://www.gurufocus.com/news/1134012/agilent-...  GuruFocus 2020-05-12   \n",
       "\n",
       "  stock                                             tokens  \\\n",
       "0     A  ['Agilent', 'Technologies', 'Announces', 'Pric...   \n",
       "1     A  ['Agilent', '(', 'A', ')', 'Gears', 'Up', 'for...   \n",
       "2     A  ['J.P.', 'Morgan', 'Asset', 'Management', 'Ann...   \n",
       "3     A  ['Pershing', 'Square', 'Capital', 'Management'...   \n",
       "4     A  ['Agilent', 'Awards', 'Trilogy', 'Sciences', '...   \n",
       "\n",
       "                                   normalized_tokens  \\\n",
       "0  ['agilent', 'technologies', 'announces', 'pric...   \n",
       "1  ['agilent', 'a', 'gears', 'up', 'for', 'q2', '...   \n",
       "2  ['morgan', 'asset', 'management', 'announces',...   \n",
       "3  ['pershing', 'square', 'capital', 'management'...   \n",
       "4  ['agilent', 'awards', 'trilogy', 'sciences', '...   \n",
       "\n",
       "                                     filtered_tokens  \\\n",
       "0  ['agilent', 'technologies', 'announces', 'pric...   \n",
       "1    ['agilent', 'gears', 'q2', 'earnings', 'cards']   \n",
       "2  ['morgan', 'asset', 'management', 'announces',...   \n",
       "3  ['pershing', 'square', 'capital', 'management'...   \n",
       "4  ['agilent', 'awards', 'trilogy', 'sciences', '...   \n",
       "\n",
       "                                              lemmas  sentiment_score  ...  \\\n",
       "0  ['agilent', 'technology', 'announces', 'pricin...           0.0000  ...   \n",
       "1      ['agilent', 'gear', 'q2', 'earnings', 'card']           0.0000  ...   \n",
       "2  ['morgan', 'asset', 'management', 'announces',...           0.3612  ...   \n",
       "3  ['pershing', 'square', 'capital', 'management'...           0.0000  ...   \n",
       "4  ['agilent', 'award', 'trilogy', 'science', 'go...           0.4588  ...   \n",
       "\n",
       "  sector_Industrials  sector_Miscellaneous sector_Real Estate  \\\n",
       "0               True                 False              False   \n",
       "1               True                 False              False   \n",
       "2               True                 False              False   \n",
       "3               True                 False              False   \n",
       "4               True                 False              False   \n",
       "\n",
       "   sector_Technology sector_Telecommunications sector_Utilities  \\\n",
       "0              False                     False            False   \n",
       "1              False                     False            False   \n",
       "2              False                     False            False   \n",
       "3              False                     False            False   \n",
       "4              False                     False            False   \n",
       "\n",
       "   recommendation_label  publisher_label  country_label  industry_label  \n",
       "0                     1                4             45              18  \n",
       "1                     1               16             45              18  \n",
       "2                     1                4             45              18  \n",
       "3                     1                4             45              18  \n",
       "4                     1                4             45              18  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917439f-655f-4d23-9a7b-7690f82b1bb4",
   "metadata": {
    "id": "e917439f-655f-4d23-9a7b-7690f82b1bb4"
   },
   "source": [
    "Create Vectors from Headlines (Vectorization Models)\n",
    "\n",
    "To generate vectors for the headline column, you can use different NLP vectorization techniques, including:\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "Count Vectorizer (Bag of Words)\n",
    "\n",
    "Word2Vec (Pre-trained Word Embeddings)\n",
    "\n",
    "Doc2Vec (Sentence-Level Embeddings)\n",
    "\n",
    "BERT Embeddings (Transformer-based Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c86ce-0360-48b7-b664-391e6d4697e7",
   "metadata": {
    "id": "372c86ce-0360-48b7-b664-391e6d4697e7"
   },
   "source": [
    "# Goal 1: Predict Buy / Hold / Sell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb53203-830b-4639-b55a-eb7c06a8cb9d",
   "metadata": {
    "id": "4eb53203-830b-4639-b55a-eb7c06a8cb9d"
   },
   "source": [
    "## Method 4: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91334140-dcc6-4041-a52f-05aaa1e1b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 14:59:17.949588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745175557.969659 1314578 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745175557.975768 1314578 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745175557.991409 1314578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745175557.991422 1314578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745175557.991425 1314578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745175557.991427 1314578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-20 14:59:17.996337: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1381ba2-6a79-422a-aa8f-de9ac5c8f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess Text + Target using HEADLINE\n",
    "text_column = 'headline'\n",
    "X_text = df_main[text_column].fillna('').astype(str)  # Clean headlines\n",
    "y = df_main.loc[X_text.index, 'recommendation_label']\n",
    "\n",
    "# Step 2: Train/test split\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_text, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_text)\n",
    "X_test_vec = vectorizer.transform(X_test_text)\n",
    "\n",
    "X_train_dense = X_train_vec.toarray().astype(\"float32\")\n",
    "X_test_dense = X_test_vec.toarray().astype(\"float32\")\n",
    "\n",
    "# Step 4: One-hot encode labels\n",
    "y_train = to_categorical(y_train_raw)\n",
    "y_test = to_categorical(y_test_raw)\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc7917d-ba9a-4afe-bfe5-a8d0edf5b765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1745175605.401869 1314578 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11437 MB memory:  -> device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:04:00.0, compute capability: 6.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">128,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m128,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,579</span> (533.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m136,579\u001b[0m (533.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,579</span> (533.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m136,579\u001b[0m (533.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation=\"relu\", input_shape=(X_train_vec.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd315d73-21b1-4604-8da1-c143b685d26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745175614.374946 1314712 service.cc:152] XLA service 0x14c0f00051d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745175614.374978 1314712 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-12GB, Compute Capability 6.0\n",
      "2025-04-20 15:00:14.405627: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1745175614.582597 1314712 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  102/46139\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 1ms/step - accuracy: 0.7570 - loss: 0.8353   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745175615.915103 1314712 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46139/46139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2ms/step - accuracy: 0.9238 - loss: 0.2601 - val_accuracy: 0.9449 - val_loss: 0.1920\n",
      "Epoch 2/5\n",
      "\u001b[1m46139/46139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2ms/step - accuracy: 0.9410 - loss: 0.2054 - val_accuracy: 0.9479 - val_loss: 0.1827\n",
      "Epoch 3/5\n",
      "\u001b[1m46139/46139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2ms/step - accuracy: 0.9439 - loss: 0.1977 - val_accuracy: 0.9486 - val_loss: 0.1801\n",
      "Epoch 4/5\n",
      "\u001b[1m46139/46139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 2ms/step - accuracy: 0.9455 - loss: 0.1937 - val_accuracy: 0.9494 - val_loss: 0.1780\n",
      "Epoch 5/5\n",
      "\u001b[1m46139/46139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2ms/step - accuracy: 0.9457 - loss: 0.1927 - val_accuracy: 0.9512 - val_loss: 0.1739\n"
     ]
    }
   ],
   "source": [
    "# Early stopping to reduce overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_dense, y_train,\n",
    "    validation_data=(X_test_dense, y_test),\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b90b2ec3-8381-469b-a107-c0b9a8bd913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11535/11535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 902us/step\n",
      "MLP Model Performance on Headlines:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.72      0.82     36913\n",
      "           1       0.95      0.99      0.97    317698\n",
      "           2       0.94      0.62      0.75     14501\n",
      "\n",
      "    accuracy                           0.95    369112\n",
      "   macro avg       0.95      0.78      0.85    369112\n",
      "weighted avg       0.95      0.95      0.95    369112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and report\n",
    "y_pred_probs = model.predict(X_test_dense)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"MLP Model Performance on Headlines:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b221b1f-278d-4129-a5b9-04ced495dde4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b221b1f-278d-4129-a5b9-04ced495dde4",
    "outputId": "906d736f-f180-4fa8-d246-1724e70c5b51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as keras_mlp_recommendation_model.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(\"keras_mlp_recommendation_model.h5\")\n",
    "print(\"Model saved as keras_mlp_recommendation_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6a5c0-a32f-4aec-8278-542ae2840539",
   "metadata": {
    "id": "c4d6a5c0-a32f-4aec-8278-542ae2840539"
   },
   "source": [
    "## Method 5: LSTM for Text-Based Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a21aab1a-15ba-4439-94b2-b66f34ac1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8b9875-8b9f-44a0-9339-911bf3a84e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use raw headlines\n",
    "texts = df_main[\"headline\"].fillna(\"\")\n",
    "labels = df_main[\"recommendation_label\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train_text, X_test_text, y_train_raw, y_test_raw = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8d3504-598d-4105-84f2-231fdc3b9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target\n",
    "y_train = to_categorical(y_train_raw)\n",
    "y_test = to_categorical(y_test_raw)\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a627f28-9177-43bd-9c74-0614d64db023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "# Convert to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 30\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c0a210a-1782-4a6d-8436-70f15edd3aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define LSTM model\n",
    "embedding_dim = 100\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=embedding_dim, input_length=max_len),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2186201b-0ab4-44b4-a8cf-4d9ceaedf0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 13ms/step - accuracy: 0.9171 - loss: 0.2817 - val_accuracy: 0.9822 - val_loss: 0.0616\n",
      "Epoch 2/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.9839 - loss: 0.0567 - val_accuracy: 0.9860 - val_loss: 0.0480\n",
      "Epoch 3/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.9878 - loss: 0.0412 - val_accuracy: 0.9883 - val_loss: 0.0415\n",
      "Epoch 4/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.9912 - loss: 0.0308 - val_accuracy: 0.9896 - val_loss: 0.0390\n",
      "Epoch 5/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.9933 - loss: 0.0241 - val_accuracy: 0.9903 - val_loss: 0.0384\n",
      "Epoch 6/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.9947 - loss: 0.0192 - val_accuracy: 0.9916 - val_loss: 0.0351\n",
      "Epoch 7/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.9958 - loss: 0.0154 - val_accuracy: 0.9914 - val_loss: 0.0366\n",
      "Epoch 8/10\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 13ms/step - accuracy: 0.9964 - loss: 0.0128 - val_accuracy: 0.9921 - val_loss: 0.0371\n"
     ]
    }
   ],
   "source": [
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=512,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba61bb89-632e-4615-9215-7dc42e211d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11535/11535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step\n",
      "LSTM Model Performance on Headlines:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     36913\n",
      "           1       0.99      1.00      1.00    317698\n",
      "           2       0.97      0.95      0.96     14501\n",
      "\n",
      "    accuracy                           0.99    369112\n",
      "   macro avg       0.98      0.97      0.98    369112\n",
      "weighted avg       0.99      0.99      0.99    369112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Report\n",
    "print(\"LSTM Model Performance on Headlines:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbc0181f-faf1-48a1-988d-ef0eb7d3188e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbc0181f-faf1-48a1-988d-ef0eb7d3188e",
    "outputId": "7439b7b9-a686-4eb7-b34d-e8ac4ea02b64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as lstm_text_recommendation_model.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(\"lstm_text_recommendation_model.h5\")\n",
    "print(\"Model saved as lstm_text_recommendation_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0553c-1a8e-48cb-9c33-9ba0ad8efa3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fbadd7f-a061-4df3-a22f-86930186ad97",
   "metadata": {},
   "source": [
    "# Model 6: Custom Transformer Model for Financial News Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ca93b46-a669-42e9-aa47-9b591166eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LayerNormalization, Dense, Dropout, Embedding, Layer, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f3ff5ad-5b22-40ba-a010-854f4bda709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.split_heads(self.wq(q), batch_size)\n",
    "        k = self.split_heads(self.wk(k), batch_size)\n",
    "        v = self.split_heads(self.wv(v), batch_size)\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.num_heads * self.depth))\n",
    "        return self.dense(concat_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa650bd9-4daa-4d5f-93a4-9414f316cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        attn_output = self.att(x, x, x)\n",
    "        out1 = self.layernorm1(x + self.dropout1(attn_output))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + self.dropout2(ffn_output))\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, num_classes, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(maximum_position_encoding, d_model)\n",
    "        self.enc_layers = [TransformerBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.global_avg_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        for enc_layer in self.enc_layers:\n",
    "            x = enc_layer(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(tf.range(position)[:, tf.newaxis], tf.range(d_model)[tf.newaxis, :], d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        return tf.cast(tf.concat([sines, cosines], axis=-1)[tf.newaxis, ...], tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        pos = tf.cast(pos, tf.float32)\n",
    "        i = tf.cast(i, tf.float32)\n",
    "        angle_rates = 1 / tf.pow(10000., (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return pos * angle_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba5b7554-5476-4505-83db-044401f9fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 4ms/step - accuracy: 0.9458 - loss: 0.1437 - val_accuracy: 0.9812 - val_loss: 0.0520\n",
      "Epoch 2/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 4ms/step - accuracy: 0.9868 - loss: 0.0418 - val_accuracy: 0.9971 - val_loss: 0.0122\n",
      "Epoch 3/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0289 - val_accuracy: 0.9944 - val_loss: 0.0167\n",
      "Epoch 4/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 4ms/step - accuracy: 0.9925 - loss: 0.0254 - val_accuracy: 0.9974 - val_loss: 0.0100\n",
      "Epoch 5/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 4ms/step - accuracy: 0.9927 - loss: 0.0250 - val_accuracy: 0.9986 - val_loss: 0.0052\n",
      "Epoch 6/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 5ms/step - accuracy: 0.9929 - loss: 0.0240 - val_accuracy: 0.9943 - val_loss: 0.0196\n",
      "Epoch 7/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 5ms/step - accuracy: 0.9929 - loss: 0.0241 - val_accuracy: 0.9964 - val_loss: 0.0142\n",
      "Epoch 8/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m510s\u001b[0m 5ms/step - accuracy: 0.9926 - loss: 0.0249 - val_accuracy: 0.9950 - val_loss: 0.0188\n",
      "Epoch 9/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.0241 - val_accuracy: 0.9965 - val_loss: 0.0117\n",
      "Epoch 10/10\n",
      "\u001b[1m107223/107223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 5ms/step - accuracy: 0.9926 - loss: 0.0246 - val_accuracy: 0.9967 - val_loss: 0.0141\n",
      "\u001b[1m11535/11535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0582\n",
      "Transformer Test Accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Split the dataset manually\n",
    "train_df, test_df = train_test_split(df_main, test_size=0.2, stratify=df_main['recommendation_label'], random_state=42)\n",
    "\n",
    "# Tokenize and pad\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['headline'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df['headline'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['headline'])\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=50, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=50, padding='post')\n",
    "\n",
    "y_train = train_df['recommendation_label'].values\n",
    "y_test = test_df['recommendation_label'].values\n",
    "\n",
    "# Oversample\n",
    "ros = RandomOverSampler()\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define and train transformer model\n",
    "model = TransformerModel(\n",
    "    num_layers=2,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    dff=128,\n",
    "    input_vocab_size=len(tokenizer.word_index) + 1,\n",
    "    maximum_position_encoding=50,\n",
    "    num_classes=3,  # update if different\n",
    "    rate=0.1\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_resampled, y_train_resampled, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Transformer Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422be0c-58e1-41e9-b5b9-ab7affb13266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b00742-907d-4678-91f4-244b794a7f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
